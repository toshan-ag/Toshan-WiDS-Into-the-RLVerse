{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([[1,2],\n",
    "                [2,2],\n",
    "                [3,3],\n",
    "                [4,5]], dtype=torch.float32)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5) =  -0.6317664384841919\n",
      "epoch 1 : w =  Parameter containing:\n",
      "tensor([[0.0651]], requires_grad=True)  loss =  31.546764373779297\n",
      "epoch 2 : w =  Parameter containing:\n",
      "tensor([[0.3213]], requires_grad=True)  loss =  21.962167739868164\n",
      "epoch 3 : w =  Parameter containing:\n",
      "tensor([[0.5350]], requires_grad=True)  loss =  15.311190605163574\n",
      "epoch 4 : w =  Parameter containing:\n",
      "tensor([[0.7132]], requires_grad=True)  loss =  10.695783615112305\n",
      "epoch 5 : w =  Parameter containing:\n",
      "tensor([[0.8618]], requires_grad=True)  loss =  7.492823600769043\n",
      "epoch 6 : w =  Parameter containing:\n",
      "tensor([[0.9857]], requires_grad=True)  loss =  5.269931316375732\n",
      "epoch 7 : w =  Parameter containing:\n",
      "tensor([[1.0892]], requires_grad=True)  loss =  3.7270889282226562\n",
      "epoch 8 : w =  Parameter containing:\n",
      "tensor([[1.1756]], requires_grad=True)  loss =  2.656123399734497\n",
      "epoch 9 : w =  Parameter containing:\n",
      "tensor([[1.2478]], requires_grad=True)  loss =  1.9125850200653076\n",
      "epoch 10 : w =  Parameter containing:\n",
      "tensor([[1.3081]], requires_grad=True)  loss =  1.396242380142212\n",
      "epoch 11 : w =  Parameter containing:\n",
      "tensor([[1.3585]], requires_grad=True)  loss =  1.0375499725341797\n",
      "epoch 12 : w =  Parameter containing:\n",
      "tensor([[1.4007]], requires_grad=True)  loss =  0.7882501482963562\n",
      "epoch 13 : w =  Parameter containing:\n",
      "tensor([[1.4361]], requires_grad=True)  loss =  0.6148579716682434\n",
      "epoch 14 : w =  Parameter containing:\n",
      "tensor([[1.4657]], requires_grad=True)  loss =  0.49413830041885376\n",
      "epoch 15 : w =  Parameter containing:\n",
      "tensor([[1.4906]], requires_grad=True)  loss =  0.40997064113616943\n",
      "epoch 16 : w =  Parameter containing:\n",
      "tensor([[1.5115]], requires_grad=True)  loss =  0.35116708278656006\n",
      "epoch 17 : w =  Parameter containing:\n",
      "tensor([[1.5291]], requires_grad=True)  loss =  0.30996543169021606\n",
      "epoch 18 : w =  Parameter containing:\n",
      "tensor([[1.5440]], requires_grad=True)  loss =  0.2809804677963257\n",
      "epoch 19 : w =  Parameter containing:\n",
      "tensor([[1.5566]], requires_grad=True)  loss =  0.2604740262031555\n",
      "epoch 20 : w =  Parameter containing:\n",
      "tensor([[1.5672]], requires_grad=True)  loss =  0.24585355818271637\n",
      "epoch 21 : w =  Parameter containing:\n",
      "tensor([[1.5763]], requires_grad=True)  loss =  0.23531952500343323\n",
      "epoch 22 : w =  Parameter containing:\n",
      "tensor([[1.5841]], requires_grad=True)  loss =  0.22762306034564972\n",
      "epoch 23 : w =  Parameter containing:\n",
      "tensor([[1.5907]], requires_grad=True)  loss =  0.22189795970916748\n",
      "epoch 24 : w =  Parameter containing:\n",
      "tensor([[1.5964]], requires_grad=True)  loss =  0.21754315495491028\n",
      "epoch 25 : w =  Parameter containing:\n",
      "tensor([[1.6014]], requires_grad=True)  loss =  0.2141413688659668\n",
      "epoch 26 : w =  Parameter containing:\n",
      "tensor([[1.6057]], requires_grad=True)  loss =  0.2114030420780182\n",
      "epoch 27 : w =  Parameter containing:\n",
      "tensor([[1.6095]], requires_grad=True)  loss =  0.20912747085094452\n",
      "epoch 28 : w =  Parameter containing:\n",
      "tensor([[1.6128]], requires_grad=True)  loss =  0.20717525482177734\n",
      "epoch 29 : w =  Parameter containing:\n",
      "tensor([[1.6158]], requires_grad=True)  loss =  0.20544946193695068\n",
      "epoch 30 : w =  Parameter containing:\n",
      "tensor([[1.6184]], requires_grad=True)  loss =  0.20388315618038177\n",
      "epoch 31 : w =  Parameter containing:\n",
      "tensor([[1.6208]], requires_grad=True)  loss =  0.2024296522140503\n",
      "epoch 32 : w =  Parameter containing:\n",
      "tensor([[1.6230]], requires_grad=True)  loss =  0.20105654001235962\n",
      "epoch 33 : w =  Parameter containing:\n",
      "tensor([[1.6250]], requires_grad=True)  loss =  0.19974163174629211\n",
      "epoch 34 : w =  Parameter containing:\n",
      "tensor([[1.6268]], requires_grad=True)  loss =  0.1984691172838211\n",
      "epoch 35 : w =  Parameter containing:\n",
      "tensor([[1.6286]], requires_grad=True)  loss =  0.19722804427146912\n",
      "epoch 36 : w =  Parameter containing:\n",
      "tensor([[1.6302]], requires_grad=True)  loss =  0.19601130485534668\n",
      "epoch 37 : w =  Parameter containing:\n",
      "tensor([[1.6317]], requires_grad=True)  loss =  0.1948131024837494\n",
      "epoch 38 : w =  Parameter containing:\n",
      "tensor([[1.6331]], requires_grad=True)  loss =  0.19363027811050415\n",
      "epoch 39 : w =  Parameter containing:\n",
      "tensor([[1.6345]], requires_grad=True)  loss =  0.19245988130569458\n",
      "epoch 40 : w =  Parameter containing:\n",
      "tensor([[1.6359]], requires_grad=True)  loss =  0.19130045175552368\n",
      "epoch 41 : w =  Parameter containing:\n",
      "tensor([[1.6372]], requires_grad=True)  loss =  0.19015051424503326\n",
      "epoch 42 : w =  Parameter containing:\n",
      "tensor([[1.6384]], requires_grad=True)  loss =  0.18900954723358154\n",
      "epoch 43 : w =  Parameter containing:\n",
      "tensor([[1.6396]], requires_grad=True)  loss =  0.1878766119480133\n",
      "epoch 44 : w =  Parameter containing:\n",
      "tensor([[1.6408]], requires_grad=True)  loss =  0.18675117194652557\n",
      "epoch 45 : w =  Parameter containing:\n",
      "tensor([[1.6420]], requires_grad=True)  loss =  0.18563327193260193\n",
      "epoch 46 : w =  Parameter containing:\n",
      "tensor([[1.6432]], requires_grad=True)  loss =  0.184522345662117\n",
      "epoch 47 : w =  Parameter containing:\n",
      "tensor([[1.6443]], requires_grad=True)  loss =  0.18341851234436035\n",
      "epoch 48 : w =  Parameter containing:\n",
      "tensor([[1.6454]], requires_grad=True)  loss =  0.1823214888572693\n",
      "epoch 49 : w =  Parameter containing:\n",
      "tensor([[1.6465]], requires_grad=True)  loss =  0.1812310814857483\n",
      "epoch 50 : w =  Parameter containing:\n",
      "tensor([[1.6476]], requires_grad=True)  loss =  0.18014732003211975\n",
      "epoch 51 : w =  Parameter containing:\n",
      "tensor([[1.6487]], requires_grad=True)  loss =  0.17907004058361053\n",
      "epoch 52 : w =  Parameter containing:\n",
      "tensor([[1.6498]], requires_grad=True)  loss =  0.177999347448349\n",
      "epoch 53 : w =  Parameter containing:\n",
      "tensor([[1.6509]], requires_grad=True)  loss =  0.17693500220775604\n",
      "epoch 54 : w =  Parameter containing:\n",
      "tensor([[1.6519]], requires_grad=True)  loss =  0.17587701976299286\n",
      "epoch 55 : w =  Parameter containing:\n",
      "tensor([[1.6530]], requires_grad=True)  loss =  0.1748254895210266\n",
      "epoch 56 : w =  Parameter containing:\n",
      "tensor([[1.6540]], requires_grad=True)  loss =  0.17378020286560059\n",
      "epoch 57 : w =  Parameter containing:\n",
      "tensor([[1.6551]], requires_grad=True)  loss =  0.17274120450019836\n",
      "epoch 58 : w =  Parameter containing:\n",
      "tensor([[1.6561]], requires_grad=True)  loss =  0.171708345413208\n",
      "epoch 59 : w =  Parameter containing:\n",
      "tensor([[1.6572]], requires_grad=True)  loss =  0.17068184912204742\n",
      "epoch 60 : w =  Parameter containing:\n",
      "tensor([[1.6582]], requires_grad=True)  loss =  0.1696612536907196\n",
      "epoch 61 : w =  Parameter containing:\n",
      "tensor([[1.6592]], requires_grad=True)  loss =  0.168646901845932\n",
      "epoch 62 : w =  Parameter containing:\n",
      "tensor([[1.6602]], requires_grad=True)  loss =  0.16763858497142792\n",
      "epoch 63 : w =  Parameter containing:\n",
      "tensor([[1.6613]], requires_grad=True)  loss =  0.16663634777069092\n",
      "epoch 64 : w =  Parameter containing:\n",
      "tensor([[1.6623]], requires_grad=True)  loss =  0.16564004123210907\n",
      "epoch 65 : w =  Parameter containing:\n",
      "tensor([[1.6633]], requires_grad=True)  loss =  0.16464966535568237\n",
      "epoch 66 : w =  Parameter containing:\n",
      "tensor([[1.6643]], requires_grad=True)  loss =  0.1636652946472168\n",
      "epoch 67 : w =  Parameter containing:\n",
      "tensor([[1.6653]], requires_grad=True)  loss =  0.16268673539161682\n",
      "epoch 68 : w =  Parameter containing:\n",
      "tensor([[1.6663]], requires_grad=True)  loss =  0.1617140769958496\n",
      "epoch 69 : w =  Parameter containing:\n",
      "tensor([[1.6673]], requires_grad=True)  loss =  0.1607472449541092\n",
      "epoch 70 : w =  Parameter containing:\n",
      "tensor([[1.6683]], requires_grad=True)  loss =  0.1597861349582672\n",
      "epoch 71 : w =  Parameter containing:\n",
      "tensor([[1.6693]], requires_grad=True)  loss =  0.15883082151412964\n",
      "epoch 72 : w =  Parameter containing:\n",
      "tensor([[1.6703]], requires_grad=True)  loss =  0.15788111090660095\n",
      "epoch 73 : w =  Parameter containing:\n",
      "tensor([[1.6713]], requires_grad=True)  loss =  0.1569371521472931\n",
      "epoch 74 : w =  Parameter containing:\n",
      "tensor([[1.6723]], requires_grad=True)  loss =  0.1559988558292389\n",
      "epoch 75 : w =  Parameter containing:\n",
      "tensor([[1.6732]], requires_grad=True)  loss =  0.15506628155708313\n",
      "epoch 76 : w =  Parameter containing:\n",
      "tensor([[1.6742]], requires_grad=True)  loss =  0.15413911640644073\n",
      "epoch 77 : w =  Parameter containing:\n",
      "tensor([[1.6752]], requires_grad=True)  loss =  0.15321756899356842\n",
      "epoch 78 : w =  Parameter containing:\n",
      "tensor([[1.6762]], requires_grad=True)  loss =  0.15230152010917664\n",
      "epoch 79 : w =  Parameter containing:\n",
      "tensor([[1.6771]], requires_grad=True)  loss =  0.15139086544513702\n",
      "epoch 80 : w =  Parameter containing:\n",
      "tensor([[1.6781]], requires_grad=True)  loss =  0.1504858285188675\n",
      "epoch 81 : w =  Parameter containing:\n",
      "tensor([[1.6791]], requires_grad=True)  loss =  0.14958606660366058\n",
      "epoch 82 : w =  Parameter containing:\n",
      "tensor([[1.6800]], requires_grad=True)  loss =  0.14869172871112823\n",
      "epoch 83 : w =  Parameter containing:\n",
      "tensor([[1.6810]], requires_grad=True)  loss =  0.14780275523662567\n",
      "epoch 84 : w =  Parameter containing:\n",
      "tensor([[1.6819]], requires_grad=True)  loss =  0.14691896736621857\n",
      "epoch 85 : w =  Parameter containing:\n",
      "tensor([[1.6829]], requires_grad=True)  loss =  0.14604055881500244\n",
      "epoch 86 : w =  Parameter containing:\n",
      "tensor([[1.6838]], requires_grad=True)  loss =  0.14516742527484894\n",
      "epoch 87 : w =  Parameter containing:\n",
      "tensor([[1.6848]], requires_grad=True)  loss =  0.14429950714111328\n",
      "epoch 88 : w =  Parameter containing:\n",
      "tensor([[1.6857]], requires_grad=True)  loss =  0.14343670010566711\n",
      "epoch 89 : w =  Parameter containing:\n",
      "tensor([[1.6867]], requires_grad=True)  loss =  0.14257913827896118\n",
      "epoch 90 : w =  Parameter containing:\n",
      "tensor([[1.6876]], requires_grad=True)  loss =  0.14172664284706116\n",
      "epoch 91 : w =  Parameter containing:\n",
      "tensor([[1.6886]], requires_grad=True)  loss =  0.1408793330192566\n",
      "epoch 92 : w =  Parameter containing:\n",
      "tensor([[1.6895]], requires_grad=True)  loss =  0.14003697037696838\n",
      "epoch 93 : w =  Parameter containing:\n",
      "tensor([[1.6904]], requires_grad=True)  loss =  0.13919974863529205\n",
      "epoch 94 : w =  Parameter containing:\n",
      "tensor([[1.6913]], requires_grad=True)  loss =  0.13836757838726044\n",
      "epoch 95 : w =  Parameter containing:\n",
      "tensor([[1.6923]], requires_grad=True)  loss =  0.1375402808189392\n",
      "epoch 96 : w =  Parameter containing:\n",
      "tensor([[1.6932]], requires_grad=True)  loss =  0.13671784102916718\n",
      "epoch 97 : w =  Parameter containing:\n",
      "tensor([[1.6941]], requires_grad=True)  loss =  0.13590046763420105\n",
      "epoch 98 : w =  Parameter containing:\n",
      "tensor([[1.6950]], requires_grad=True)  loss =  0.1350879818201065\n",
      "epoch 99 : w =  Parameter containing:\n",
      "tensor([[1.6959]], requires_grad=True)  loss =  0.13428035378456116\n",
      "epoch 100 : w =  Parameter containing:\n",
      "tensor([[1.6968]], requires_grad=True)  loss =  0.13347749412059784\n",
      "Prediction after training : f(5) =  9.375541687011719\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1],\n",
    "                [2],\n",
    "                [3],\n",
    "                [4]], dtype=torch.float32)\n",
    "y=torch.tensor([[2],\n",
    "                [4],\n",
    "                [6],\n",
    "                [8]], dtype=torch.float32)\n",
    "\n",
    "no_of_samples, no_of_features = x.shape\n",
    "\n",
    "input_size = no_of_features\n",
    "output_size = 1\n",
    "\n",
    "#model = nn.Linear(input_size, output_size)\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "x_test = torch.tensor([5], dtype=torch.float32)\n",
    "print(\"Prediction before training : f(5) = \",model(x_test).item())\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "no_of_iters = 100\n",
    "\n",
    "loss = nn.MSELoss() # We are defining a MSE Loss function using nn, because Linear Regression uses MSE Loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #Stochastic Gradient Descent\n",
    "\n",
    "for epoch in range(no_of_iters):\n",
    "    # Prediction\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Loss\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    # Gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    [w, b] = model.parameters() #w is wights and b is optional bias\n",
    "    print(\"epoch\", epoch + 1, \": w = \", w, \" loss = \", l.item())\n",
    "\n",
    "print(\"Prediction after training : f(5) = \", model(x_test).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.lin(x)\n",
    "'''\n",
    "In PyTorch, defining the forward function within a custom neural network class, such as the one inheriting from nn.Module, is crucial for several reasons:\n",
    "\n",
    "Forward Pass Definition:\n",
    "\n",
    "The primary purpose of the forward method is to define how the input data flows through the network layers during the forward pass. It specifies the sequence of operations that transform the input into the output.\n",
    "Automatic Differentiation:\n",
    "\n",
    "PyTorch uses dynamic computational graphs for automatic differentiation. When you call the forward method, PyTorch builds the computation graph, which is essential for computing gradients during the backward pass (gradient computation for training).\n",
    "Backward Pass Computation:\n",
    "\n",
    "The forward method, along with the autograd system in PyTorch, allows the framework to automatically track operations during the forward pass. This information is then used to compute gradients during the backward pass, which is necessary for training the model.\n",
    "Parameter Update:\n",
    "\n",
    "During training, the parameters (weights and biases) of the neural network are updated based on the gradients computed during the backward pass. The forward method is critical for establishing the connections between input, parameters, and output, enabling the computation of these gradients.\n",
    "Model Behavior Specification:\n",
    "\n",
    "The forward method encapsulates the behavior of the model. When you call the model instance with input data, such as output = model(input), it's the forward method that gets executed, defining how the model processes the input.\n",
    "Layer Composition:\n",
    "\n",
    "For models with multiple layers or submodules, the forward method specifies the order in which these layers are applied to the input data. This ensures that the data flows correctly through the network.\n",
    "Custom Operations:\n",
    "\n",
    "If your model includes custom operations or non-linearities, you can include them in the forward method. This allows you to define complex architectures beyond simple layer stacking.\n",
    "Code Clarity:\n",
    "\n",
    "Explicitly defining the forward method makes your code more readable and intuitive. It clearly indicates the sequence of operations that make up the forward pass, making it easier for others (and yourself) to understand the model's behavior.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
