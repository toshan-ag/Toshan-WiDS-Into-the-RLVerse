{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([1,2,3,4], dtype=np.float32)\n",
    "y=np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model prediction\n",
    "\n",
    "def forward(x):\n",
    "    return w*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return np.mean((y - y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient\n",
    "def gradient(x,y,y_pred):\n",
    "    return np.dot(2*x,(y_pred-y))/len(x) #because, dLoss/dw = 2x(y-ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5) =  0.0\n",
      "30.0\n",
      "epoch 1 : w =  0.3  loss =  30.0\n",
      "21.675\n",
      "epoch 2 : w =  0.555  loss =  21.675\n",
      "15.660187\n",
      "epoch 3 : w =  0.772  loss =  15.66\n",
      "11.314486\n",
      "epoch 4 : w =  0.956  loss =  11.314\n",
      "8.174716\n",
      "epoch 5 : w =  1.113  loss =  8.175\n",
      "5.9062324\n",
      "epoch 6 : w =  1.246  loss =  5.906\n",
      "4.267253\n",
      "epoch 7 : w =  1.359  loss =  4.267\n",
      "3.0830898\n",
      "epoch 8 : w =  1.455  loss =  3.083\n",
      "2.2275321\n",
      "epoch 9 : w =  1.537  loss =  2.228\n",
      "1.6093926\n",
      "epoch 10 : w =  1.606  loss =  1.609\n",
      "1.1627856\n",
      "epoch 11 : w =  1.665  loss =  1.163\n",
      "0.8401131\n",
      "epoch 12 : w =  1.716  loss =  0.84\n",
      "0.60698175\n",
      "epoch 13 : w =  1.758  loss =  0.607\n",
      "0.4385444\n",
      "epoch 14 : w =  1.794  loss =  0.439\n",
      "0.31684822\n",
      "epoch 15 : w =  1.825  loss =  0.317\n",
      "0.22892293\n",
      "epoch 16 : w =  1.851  loss =  0.229\n",
      "0.16539653\n",
      "epoch 17 : w =  1.874  loss =  0.165\n",
      "0.11949898\n",
      "epoch 18 : w =  1.893  loss =  0.119\n",
      "0.08633806\n",
      "epoch 19 : w =  1.909  loss =  0.086\n",
      "0.062379323\n",
      "epoch 20 : w =  1.922  loss =  0.062\n",
      "0.045069046\n",
      "epoch 21 : w =  1.934  loss =  0.045\n",
      "0.032562442\n",
      "epoch 22 : w =  1.944  loss =  0.033\n",
      "0.023526315\n",
      "epoch 23 : w =  1.952  loss =  0.024\n",
      "0.016997725\n",
      "epoch 24 : w =  1.96  loss =  0.017\n",
      "0.012280917\n",
      "epoch 25 : w =  1.966  loss =  0.012\n",
      "0.00887291\n",
      "epoch 26 : w =  1.971  loss =  0.009\n",
      "0.0064107166\n",
      "epoch 27 : w =  1.975  loss =  0.006\n",
      "0.004631734\n",
      "epoch 28 : w =  1.979  loss =  0.005\n",
      "0.0033464201\n",
      "epoch 29 : w =  1.982  loss =  0.003\n",
      "0.0024177833\n",
      "epoch 30 : w =  1.985  loss =  0.002\n",
      "0.0017468547\n",
      "epoch 31 : w =  1.987  loss =  0.002\n",
      "0.0012621148\n",
      "epoch 32 : w =  1.989  loss =  0.001\n",
      "0.00091188005\n",
      "epoch 33 : w =  1.991  loss =  0.001\n",
      "0.00065882097\n",
      "epoch 34 : w =  1.992  loss =  0.001\n",
      "0.00047600627\n",
      "epoch 35 : w =  1.993  loss =  0.0\n",
      "0.00034390565\n",
      "epoch 36 : w =  1.994  loss =  0.0\n",
      "0.000248478\n",
      "epoch 37 : w =  1.995  loss =  0.0\n",
      "0.00017952273\n",
      "epoch 38 : w =  1.996  loss =  0.0\n",
      "0.0001297064\n",
      "epoch 39 : w =  1.996  loss =  0.0\n",
      "9.3710754e-05\n",
      "epoch 40 : w =  1.997  loss =  0.0\n",
      "6.770495e-05\n",
      "epoch 41 : w =  1.997  loss =  0.0\n",
      "4.8919566e-05\n",
      "epoch 42 : w =  1.998  loss =  0.0\n",
      "3.5343608e-05\n",
      "epoch 43 : w =  1.998  loss =  0.0\n",
      "2.5536516e-05\n",
      "epoch 44 : w =  1.998  loss =  0.0\n",
      "1.8450231e-05\n",
      "epoch 45 : w =  1.999  loss =  0.0\n",
      "1.3328778e-05\n",
      "epoch 46 : w =  1.999  loss =  0.0\n",
      "9.631532e-06\n",
      "epoch 47 : w =  1.999  loss =  0.0\n",
      "6.9583166e-06\n",
      "epoch 48 : w =  1.999  loss =  0.0\n",
      "5.027384e-06\n",
      "epoch 49 : w =  1.999  loss =  0.0\n",
      "3.6322847e-06\n",
      "epoch 50 : w =  1.999  loss =  0.0\n",
      "2.6243997e-06\n",
      "epoch 51 : w =  1.999  loss =  0.0\n",
      "1.8964256e-06\n",
      "epoch 52 : w =  2.0  loss =  0.0\n",
      "1.3698847e-06\n",
      "epoch 53 : w =  2.0  loss =  0.0\n",
      "9.894591e-07\n",
      "epoch 54 : w =  2.0  loss =  0.0\n",
      "7.1484834e-07\n",
      "epoch 55 : w =  2.0  loss =  0.0\n",
      "5.1688625e-07\n",
      "epoch 56 : w =  2.0  loss =  0.0\n",
      "3.7306336e-07\n",
      "epoch 57 : w =  2.0  loss =  0.0\n",
      "2.6975357e-07\n",
      "epoch 58 : w =  2.0  loss =  0.0\n",
      "1.9482059e-07\n",
      "epoch 59 : w =  2.0  loss =  0.0\n",
      "1.4073338e-07\n",
      "epoch 60 : w =  2.0  loss =  0.0\n",
      "1.0175587e-07\n",
      "epoch 61 : w =  2.0  loss =  0.0\n",
      "7.338856e-08\n",
      "epoch 62 : w =  2.0  loss =  0.0\n",
      "5.3154125e-08\n",
      "epoch 63 : w =  2.0  loss =  0.0\n",
      "3.8369308e-08\n",
      "epoch 64 : w =  2.0  loss =  0.0\n",
      "2.7700096e-08\n",
      "epoch 65 : w =  2.0  loss =  0.0\n",
      "1.9992076e-08\n",
      "epoch 66 : w =  2.0  loss =  0.0\n",
      "1.4520101e-08\n",
      "epoch 67 : w =  2.0  loss =  0.0\n",
      "1.044835e-08\n",
      "epoch 68 : w =  2.0  loss =  0.0\n",
      "7.552629e-09\n",
      "epoch 69 : w =  2.0  loss =  0.0\n",
      "5.4534013e-09\n",
      "epoch 70 : w =  2.0  loss =  0.0\n",
      "3.929017e-09\n",
      "epoch 71 : w =  2.0  loss =  0.0\n",
      "2.8666136e-09\n",
      "epoch 72 : w =  2.0  loss =  0.0\n",
      "2.0563e-09\n",
      "epoch 73 : w =  2.0  loss =  0.0\n",
      "1.4790231e-09\n",
      "epoch 74 : w =  2.0  loss =  0.0\n",
      "1.0658141e-09\n",
      "epoch 75 : w =  2.0  loss =  0.0\n",
      "7.718661e-10\n",
      "epoch 76 : w =  2.0  loss =  0.0\n",
      "5.69532e-10\n",
      "epoch 77 : w =  2.0  loss =  0.0\n",
      "4.0706993e-10\n",
      "epoch 78 : w =  2.0  loss =  0.0\n",
      "2.8819613e-10\n",
      "epoch 79 : w =  2.0  loss =  0.0\n",
      "2.1679014e-10\n",
      "epoch 80 : w =  2.0  loss =  0.0\n",
      "1.5229773e-10\n",
      "epoch 81 : w =  2.0  loss =  0.0\n",
      "1.09139364e-10\n",
      "epoch 82 : w =  2.0  loss =  0.0\n",
      "7.712586e-11\n",
      "epoch 83 : w =  2.0  loss =  0.0\n",
      "5.5894844e-11\n",
      "epoch 84 : w =  2.0  loss =  0.0\n",
      "4.2632564e-11\n",
      "epoch 85 : w =  2.0  loss =  0.0\n",
      "3.1167957e-11\n",
      "epoch 86 : w =  2.0  loss =  0.0\n",
      "2.0307311e-11\n",
      "epoch 87 : w =  2.0  loss =  0.0\n",
      "1.5347723e-11\n",
      "epoch 88 : w =  2.0  loss =  0.0\n",
      "1.10986775e-11\n",
      "epoch 89 : w =  2.0  loss =  0.0\n",
      "8.8284935e-12\n",
      "epoch 90 : w =  2.0  loss =  0.0\n",
      "6.8212103e-12\n",
      "epoch 91 : w =  2.0  loss =  0.0\n",
      "3.5953462e-12\n",
      "epoch 92 : w =  2.0  loss =  0.0\n",
      "3.5953462e-12\n",
      "epoch 93 : w =  2.0  loss =  0.0\n",
      "2.7746694e-12\n",
      "epoch 94 : w =  2.0  loss =  0.0\n",
      "1.7053026e-12\n",
      "epoch 95 : w =  2.0  loss =  0.0\n",
      "8.9883656e-13\n",
      "epoch 96 : w =  2.0  loss =  0.0\n",
      "8.9883656e-13\n",
      "epoch 97 : w =  2.0  loss =  0.0\n",
      "5.258016e-13\n",
      "epoch 98 : w =  2.0  loss =  0.0\n",
      "5.258016e-13\n",
      "epoch 99 : w =  2.0  loss =  0.0\n",
      "5.258016e-13\n",
      "epoch 100 : w =  2.0  loss =  0.0\n",
      "Prediction after training : f(5) =  9.99999914467335\n"
     ]
    }
   ],
   "source": [
    "w=0.0\n",
    "\n",
    "print(\"Prediction before training : f(5) = \",round(forward(5),3))\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "no_of_iters = 100\n",
    "\n",
    "for epoch in range(no_of_iters):\n",
    "    #prediction\n",
    "    y_pred = forward(x)\n",
    "\n",
    "    #loss\n",
    "    l = loss(y,y_pred)\n",
    "    print(l)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    dw = gradient(x,y,y_pred)\n",
    "\n",
    "    #update weights\n",
    "    w = w - learning_rate*dw\n",
    "\n",
    "    print(\"epoch\", epoch+1, \": w = \",round(float(w),3),\" loss = \",round(float(l),3))\n",
    "print(\"Prediction after training : f(5) = \",forward(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
