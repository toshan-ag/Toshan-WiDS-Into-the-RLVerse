{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Tic-Tac-Toe using TD(λ)\n",
    "In this assignment you will build an RL agent capable of playing Tic-Tac-Toe using TD(λ) algorithm and the environment simulated by you in first week.\n",
    "\n",
    "First of all copy the environment simulated by you from the first week below.\n",
    "- Note that you should also return the state of the board each time you call act function, ideally the state should be stored in a numpy array for faster implementation\n",
    "- The only input the function can take is via its own arguments and not input() function.\n",
    "\n",
    "The ideal TicTacToe environment:\n",
    "- Will take N, the size of board as an argument in its constructor.\n",
    "- Will have act function taking a single argument representing the action taken (preferably int type) and return the state of board (preferably numpy array), reward signal (float) and bool value \"done\" which is true if the game is over else false.\n",
    "- Will have reset function which resets the board and starts a new game.\n",
    "- Will give reward signal as 1 if 'X' won and -1 if 'O' won (or vice-versa) and 0 if its a draw.\n",
    "- Will take alternate calls of act function as the moves of one player.\n",
    "\n",
    "For example:\n",
    "```html\n",
    "env.reset()\n",
    "Returns ==> (array([[0., 0., 0.],[0., 0., 0.],[0., 0., 0.]]), 0, False)\n",
    "                | | | |\n",
    "        Board:  | | | |\n",
    "                | | | |\n",
    "\n",
    "env.act(4)\n",
    "Returns ==> (array([[0., 0., 0.],[0., 1.0, 0.],[0., 0., 0.]]), 0, False)\n",
    "                | | | |\n",
    "        Board:  | |X| |\n",
    "                | | | |\n",
    "\n",
    "env.act(0)\n",
    "Returns ==> (array([[-1.0, 0., 0.],[0., 1.0, 0.],[0., 0., 0.]]), 0, False)\n",
    "                |O| | |\n",
    "        Board:  | |X| |\n",
    "                | | | |\n",
    "\n",
    "env.act(7)\n",
    "Returns ==> (array([[-1.0, 0., 0.],[0., 1.0, 0.],[0., 1.0, 0.]]), 0, False)\n",
    "                |O| | |\n",
    "        Board:  | |X| |\n",
    "                | |X| |\n",
    "\n",
    "env.act(6)\n",
    "Returns ==> (array([[-1.0, 0., 0.],[0., 1.0, 0.],[-1.0, 1.0, 0.]]), 0, False)\n",
    "                |O| | |\n",
    "        Board:  | |X| |\n",
    "                |O|X| |\n",
    "\n",
    "env.act(2)\n",
    "Returns ==> (array([[-1.0, 1.0, 0.],[0., 1.0, 0.],[-1.0, 1.0, 0.]]), 1, True)\n",
    "                |O|X| |\n",
    "        Board:  | |X| |\n",
    "                |O|X| |\n",
    "\n",
    "\n",
    "```\n",
    "<hr>\n",
    "\n",
    "Note : You can change your TicTacToe environment code before using it here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import any necessary libraries here\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your TicTacToe environment class comes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self,N):\n",
    "        self.N=N\n",
    "        self.board = np.zeros((self.N,self.N))\n",
    "        self.player = 'X'\n",
    "        self.winner = ''\n",
    "        self.game_complete = False\n",
    "    # Given below is the preferable structure of act function\n",
    "    def act(self, action : int) -> tuple:     # Returns tuple of types (np.ndarray, int, bool)\n",
    "        row = (action)//(self.N)\n",
    "        col = (action)%(self.N)\n",
    "        if self.game_complete==False and self.board[row][col]==0:\n",
    "            if self.player=='X':\n",
    "                self.board[row][col] = 1\n",
    "            elif self.player=='O':\n",
    "                self.board[row][col] = -1\n",
    "            self._winnercheck()\n",
    "            self._drawcheck()\n",
    "            if self.player=='X':\n",
    "                self.player='O'\n",
    "            elif self.player=='O':\n",
    "                self.player='X'\n",
    "            #print('Move Succesful')\n",
    "            for i in self.board:\n",
    "                s=''\n",
    "                for j in i:\n",
    "                    if j==1:\n",
    "                        s+='X'\n",
    "                    elif j==-1:\n",
    "                        s+='O'\n",
    "                    else:\n",
    "                        s+=' '\n",
    "                s=\"|\" + \"|\".join(s) + \"|\"\n",
    "                print(s)\n",
    "            reward=0\n",
    "            if self.game_complete==True:\n",
    "                if self.winner=='X':\n",
    "                    reward=1\n",
    "                elif self.winner=='O':\n",
    "                    reward=-1\n",
    "            return (self.board,reward,self.game_complete)\n",
    "        else:\n",
    "            print('Invalid Move!')\n",
    "            return\n",
    "\n",
    "    def reset(self):\n",
    "        self.board =np.zeros((3,3))\n",
    "        self.player = 'X'\n",
    "        self.winner = ''\n",
    "        self.game_complete = False\n",
    "        reward=0\n",
    "        return (self.board,reward,self.game_complete)\n",
    "    \n",
    "    def _winnercheck(self):\n",
    "        temp1=False\n",
    "        for i in range(self.N):\n",
    "            if self.board[i][0]!=0 and len(np.unique(self.board[i]))==1:\n",
    "                temp1=True\n",
    "        temp2=False\n",
    "        for i in range(self.N):\n",
    "            if self.board.T[i][0]!=0 and len(np.unique(self.board.T[i]))==1:\n",
    "                temp2=True\n",
    "        if temp1 or temp2:\n",
    "            self.game_complete = True\n",
    "            self.winner = self.player\n",
    "            return True\n",
    "        temp3=False\n",
    "        if self.board[0][0]!=0:\n",
    "            for i in range(1, self.N):\n",
    "                if self.board[i][i] != self.board[0][0]:\n",
    "                    break\n",
    "            else:\n",
    "                temp3=True\n",
    "        \n",
    "        if self.board[self.N-1][0]!=0:\n",
    "            for i in range(1, self.N):\n",
    "                if self.board[self.N-i-1][i] != self.board[self.N-1][0]:\n",
    "                    break\n",
    "            else:\n",
    "                temp3=True\n",
    "        if temp3:\n",
    "            self.game_complete=True\n",
    "            self.winner=self.player\n",
    "            #print(\"hi\")\n",
    "            return True\n",
    "    def _drawcheck(self):\n",
    "        if self.game_complete==False and (0 not in self.board):\n",
    "            self.game_complete=True\n",
    "            self.winner= 'Draw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToe(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then comes the agent class which\n",
    "- Uses TD(λ) algorithm to find the optimal policies for each state\n",
    "- Stores the calculated optimal policies as a .npy file for later use\n",
    "- Calculates the average return of the itself against a random player (makes random moves on its chance) periodically during training and plot it (for example if total training iterations is 10000, then calculate average return after each 500 steps, also for average return you should calculate return atleast 5 times and then take average)\n",
    "- You can make additional functions\n",
    "\n",
    "You can store all the encountered states in a numpy array (which will have 3 dims) and then store corresponding values for that particulare state in another array (will have 1 dims) and then you can store all these arrays in a .npy file for future use, so that you don't have to train the model each time you want to play TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeagent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def decay_schedule(self, init_value, min_value, decay_ratio, max_steps, log_start=-2, log_base=10):\n",
    "        decay_steps = int(max_steps * decay_ratio)\n",
    "        rem_steps = max_steps - decay_steps\n",
    "        values = np.logspace(log_start, 0, decay_steps, base=log_base, endpoint=True)[::-1]\n",
    "        values = (values - values.min()) / (values.max() - values.min())\n",
    "        values = (init_value - min_value) * values + min_value\n",
    "        values = np.pad(values, (0, rem_steps), 'edge')\n",
    "        return values\n",
    "\n",
    "    \n",
    "    def train(self, gamma=0.9, init_alpha=0.1, min_alpha=0.01, alpha_decay_ratio=0.8,\n",
    "              init_epsilon=1.0, min_epsilon=0.1, epsilon_decay_ratio=0.9, n_episodes=10000):\n",
    "        nS, nA = 3 ** (self.env.N ** 2), (self.env.N ** 2)\n",
    "\n",
    "        Q = np.random.rand(nS, nA)\n",
    "        states=list()\n",
    "        def select_action(state_code, Q, epsilon, actions_possible):\n",
    "            if (np.random.random() > epsilon) and (np.argmax(Q[state_code]) in actions_possible):\n",
    "                a = np.argmax(Q[state_code])\n",
    "            else:\n",
    "                a = random.choice(actions_possible)\n",
    "            actions_possible.remove(a)\n",
    "            return a\n",
    "       \n",
    "\n",
    "        initial_state = np.zeros((3,3))\n",
    "        temp=tuple(initial_state.tolist())\n",
    "        states.append(temp)\n",
    "\n",
    "        alphas = self.decay_schedule(init_alpha, min_alpha, alpha_decay_ratio, n_episodes)\n",
    "        epsilons = self.decay_schedule(init_epsilon, min_epsilon, epsilon_decay_ratio, n_episodes)\n",
    "\n",
    "        for e in tqdm(range(n_episodes), leave=False):\n",
    "            state, _, done = self.env.reset()\n",
    "            \n",
    "            actions_possible = list(range(self.env.N ** 2))\n",
    "            print(actions_possible,\"hi\")\n",
    "            temp=tuple(state.tolist())\n",
    "            state_code = states.index(temp)\n",
    "            action = select_action(state_code, Q, epsilons[e], actions_possible)\n",
    "\n",
    "            while not done:\n",
    "                print(actions_possible,\"hi2\")\n",
    "                #print(action)\n",
    "                next_state, reward, done = self.env.act(action)\n",
    "                #print(next_state, done)\n",
    "                temp=tuple(next_state.tolist())\n",
    "                if temp not in states:\n",
    "                    states.append(temp)\n",
    "                    \n",
    "                temp=tuple(next_state.tolist())\n",
    "                next_state_code = states.index(temp)\n",
    "                if done:\n",
    "                    next_action = random.choice(list(range(self.env.N ** 2)))\n",
    "                else:\n",
    "                    next_action = select_action(next_state_code, Q, epsilons[e], actions_possible)\n",
    "                print(actions_possible,\"hi3\")\n",
    "                temp=tuple(state.tolist())\n",
    "                state_code = states.index(temp)\n",
    "                td_target = reward + (gamma*Q[next_state_code][next_action])\n",
    "                td_error = td_target - Q[state_code][action]\n",
    "\n",
    "                Q[state_code][action] += (alphas[e] * td_error)\n",
    "\n",
    "                state, action = next_state, next_action\n",
    "\n",
    "        V = np.max(Q, axis=1)\n",
    "        pi = lambda s: {s: a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "        return Q, V, pi, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for evaluation purposes and for your self checking the code block below after running should:\n",
    "- Initialize the agent and call the train function which trains the agent\n",
    "- Load the stored state value data\n",
    "- Start a single player game of TicTacToe which takes input from the user for moves according to the convention given below, where the trained Q values play as computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TicTacToeagent(env)\n",
    "\n",
    "Q_values, values, policy, states = agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_codes = range(len(states))  # Replace with the actual states in your environment\n",
    "policy_values = {i: policy(i) for i in state_codes}\n",
    "np.save('policy_values.npy', policy_values)\n",
    "loaded_policy_values = np.load('policy_values.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1 | 2 | 3\")\n",
    "print(\"4 | 5 | 6\")\n",
    "print(\"7 | 8 | 9\")\n",
    "'''The model should train a 3x3 TicTacToe by default, you can definitely modify the values(of N, number of iterations etc) for your convenience but training model for bigger N might take lot of time\n",
    "'''\n",
    "\n",
    "# Code Here\n",
    "def play():\n",
    "    def print_board(board):\n",
    "        for row in board:\n",
    "            s = ''\n",
    "            for cell in row:\n",
    "                if cell == 1:\n",
    "                    s += 'X'\n",
    "                elif cell == -1:\n",
    "                    s += 'O'\n",
    "                else:\n",
    "                    s += ' '\n",
    "            print(\"|\" + \"|\".join(s) + \"|\")\n",
    "\n",
    "    state, _, done = env.reset()\n",
    "    while not done:\n",
    "        print(\"\\nCurrent Board:\")\n",
    "        print_board(state)\n",
    "\n",
    "        if env.player == 'O':\n",
    "            action = int(input(\"Enter your move (1-9): \")) - 1\n",
    "        else:\n",
    "            temp = tuple(state.tolist())\n",
    "            state_code = states.index(temp)\n",
    "            action = policy(state_code)\n",
    "            print(action + 1)  # Adding 1 to match the user input\n",
    "\n",
    "        next_state, reward, done = env.act(action)\n",
    "\n",
    "        if not done:\n",
    "            state = next_state\n",
    "        else:\n",
    "            print(\"\\nGame Over!\")\n",
    "            if reward == 1:\n",
    "                print(\"Computer won!\")\n",
    "            elif reward == -1:\n",
    "                print(\"You Won!\")\n",
    "            else:\n",
    "                print(\"It's a draw!\")\n",
    "\n",
    "# Call the play function\n",
    "play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loaded_policy_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
